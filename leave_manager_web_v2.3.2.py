# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
from typing import List, Dict, Tuple, Optional
from io import BytesIO
from PIL import Image
import re

# ========================
# OCR backend: RapidOCR (ONNXRuntime)
# ========================
@st.cache_resource(show_spinner=True)
def load_ocr_reader():
    from rapidocr_onnxruntime import RapidOCR
    return RapidOCR()  # CPUã€ç´” pipï¼Œé›²ç«¯å‹å–„

def ocr_image(reader, file) -> List[str]:
    """å›å‚³å½±åƒæ–‡å­—è¡Œï¼ˆä¿¡å¿ƒ >= 0.35ï¼‰ï¼Œåšå¸¸è¦‹æ­£è¦åŒ–ã€‚"""
    img = Image.open(file).convert("RGB")
    arr = np.array(img)
    try:
        result, _ = reader(arr)   # result: list of [box, text, score]
    except Exception:
        result = []
    lines = []
    for item in (result or []):
        if not item or len(item) < 3:
            continue
        text = str(item[1]); score = float(item[2] or 0.0)
        if score >= 0.35:
            text = norm(text)
            if text:
                lines.append(text)
    return lines

# ========================
# Utils
# ========================
FULLWIDTH_DIGITS = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
CHECK_MARKS = ["âœ“", "âœ”", "âœ…", "â˜‘", "â– ", "â–¡", "V", "v", "å‹¾", "âˆš"]

def norm(s: str) -> str:
    if s is None: return ""
    s = str(s).translate(FULLWIDTH_DIGITS)
    s = s.replace("\u3000", " ").replace("ï¼", "/")
    s = re.sub(r"[ \t]+", " ", s).strip()
    return s

def has_check(s: str) -> bool:
    return any(m in s for m in CHECK_MARKS)

def find_time(s: str) -> str:
    """
    æ“·å–æ™‚é–“ï¼š09:05ã€9:5ã€9ï¼š05ã€9æ™‚05åˆ† â†’ æ¨™æº– HH:MM
    """
    x = s.replace("ï¼š", ":")
    m = re.search(r"(\d{1,2})\s*[æ™‚ç‚¹é»]\s*(\d{1,2})\s*åˆ†?", x)
    if m:
        hh = int(m.group(1)); mm = int(m.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    m2 = re.search(r"\b(\d{1,2})\s*:\s*(\d{1,2})\b", x)
    if m2:
        hh = int(m2.group(1)); mm = int(m2.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    return ""

def roc_to_date(roc_str: str, fallback_roc_year: Optional[int]=None) -> Optional[date]:
    """
    æ”¯æ´ 114/10/31ã€10/31ï¼ˆè£œå¹´ï¼‰ã€114å¹´10æœˆ31æ—¥ / 10æœˆ31æ—¥
    """
    s = norm(roc_str).replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    s = re.sub(r"[.\-]", "/", s)

    m = re.search(r"(\d{2,3})\s*/\s*(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m:
        y, mo, da = map(int, m.groups())
        try:
            return date(y + 1911, mo, da)
        except Exception:
            return None

    m2 = re.search(r"(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m2 and fallback_roc_year:
        mo, da = map(int, m2.groups())
        try:
            return date(fallback_roc_year + 1911, mo, da)
        except Exception:
            return None

    return None

def date_to_roc(d: date) -> str:
    return f"{d.year - 1911}/{d.month:02d}/{d.day:02d}"

def parse_mapping_txt(txt: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    """name_id_clean.txt â†’ (id2name, name2id)"""
    id2name, name2id = {}, {}
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        parts = re.split(r"[\t,ï¼Œ\s]+", ln)
        if len(parts) < 2: continue
        a, b = parts[0], parts[1]
        if re.fullmatch(r"\d{7}", a) and not re.fullmatch(r"\d{7}", b):
            sid, name = a, b
        elif re.fullmatch(r"\d{7}", b) and not re.fullmatch(r"\d{7}", a):
            sid, name = b, a
        else:
            sid, name = (a, b) if re.search(r"\d", a) else (b, a)
        sid, name = norm(sid), norm(name)
        if re.fullmatch(r"\d{7}", sid) and name:
            id2name[sid] = name
            name2id[name] = sid
    return id2name, name2id

# ---- å‡åˆ¥ï¼ˆå¯æ“´å……ï¼‰----
# ä½ çš„é è¨­ï¼šç—…ã€äº‹ã€ç‰¹åˆ¥ã€å…¬ã€è«–ï¼ˆå¯åœ¨ sidebar å†åŠ è‡ªè¨‚ï¼‰
DEFAULT_TYPE_PATTERNS = {
    "ç—…":   [r"ç—…"],
    "äº‹":   [r"äº‹"],
    "ç‰¹åˆ¥": [r"ç‰¹åˆ¥", r"ç‰¹ä¼‘", r"ç‰¹(?!è­¦|æ®Š)"],  # é¿å…èª¤æŠ“éå‡åˆ¥
    "å…¬":   [r"å…¬(å‡|å‡º)?"],
    "è«–":   [r"è«–"],  # è«–æ–‡/è«–å‡º/è«–å…¥
}

def detect_leave_type(text: str, extra_keywords: list[str]) -> str:
    """å›å‚³åµæ¸¬åˆ°çš„å‡åˆ¥ï¼›å„ªå…ˆä½ çš„é¡åˆ¥ï¼Œå†è©¦ä½¿ç”¨è€…è‡ªè¨‚ï¼Œæœ€å¾ŒæŠ“é€šç”¨ã€ŒXXå‡ã€æˆ–å¸¸è¦‹è©ã€‚"""
    T = norm(text)

    for canon, regs in DEFAULT_TYPE_PATTERNS.items():
        for rgx in regs:
            if re.search(rgx, T):
                return canon

    for kw in extra_keywords:
        if kw and kw in T:
            return kw

    m = re.search(r"([\u4e00-\u9fa5]{1,4})\s*å‡", T)
    if m:
        return m.group(1)

    m2 = re.search(r"(å–ª|å©š|ç”¢|è£œ|æ…°|ç—…|äº‹|å…¬|ç‰¹|è«–|å…µå½¹)", T)
    if m2:
        return m2.group(1)

    return ""

def parse_sign_lines(
    lines: List[str],
    id2name: Dict[str,str],
    fallback_roc_year: int,
    source_label: str,
    extra_type_keywords: list[str]
) -> pd.DataFrame:
    """
    å°‡ã€ŒéšŠéƒ¨/å¤§é–€ã€ç°½å‡ºå…¥ç…§ç‰‡ OCR è¡Œè½‰ç‚ºçµæ§‹è³‡æ–™ï¼š
    æ¬„ä½ï¼šsid, name, date(ROC), dir(å‡º/å…¥), time(HH:MM), type, source, raw
    """
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue

        sid = ""
        m_sid = re.search(r"(\d{7})", L)
        if m_sid: sid = m_sid.group(1)

        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname: name = mname.group(0)

        d = roc_to_date(L, fallback_roc_year=fallback_roc_year)
        time_ = find_time(L)

        dir_ = ""
        if "å‡º" in L: dir_ = "å‡º"
        if "å…¥" in L: dir_ = "å…¥"
        if not dir_ and has_check(L):
            if re.search(r"å‡º.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å‡º"
            elif re.search(r"å…¥.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å…¥"

        kind = detect_leave_type(L, extra_type_keywords)

        if any([sid, name, d, dir_, time_, kind]) or has_check(L):
            rows.append({
                "sid": sid,
                "name": name,
                "date": date_to_roc(d) if d else "",
                "dir": dir_,
                "time": time_,
                "type": kind,
                "source": source_label,
                "raw": L
            })

    df = pd.DataFrame(rows, columns=["sid","name","date","dir","time","type","source","raw"]).drop_duplicates()
    if not df.empty:
        df["name"] = df.apply(lambda r: id2name.get(r["sid"], r["name"]), axis=1)
    return df

def parse_leave_from_lines(lines: List[str], id2name: Dict[str,str],
                           name2id: Dict[str,str], fallback_roc_year: int,
                           extra_type_keywords: list[str]) -> pd.DataFrame:
    """
    å‡å–®ç…§ç‰‡ â†’ (sid, name, start, end, type)
    - å„ªå…ˆæŠ“ 7 ç¢¼å­¸è™Ÿï¼›å¦‚æœªæŠ“åˆ°ï¼Œè©¦å§“åâ†’å­¸è™Ÿ
    - æ”¯æ´å–®æ—¥æˆ–å€é–“ï¼ˆ114/10/27 ~ 114/10/29ã€æˆ– 10/27 ~ 10/29 ç”¨é è¨­å¹´è£œï¼‰
    """
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue

        sid = ""
        m = re.search(r"(\d{7})", L)
        if m: sid = m.group(1)

        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname:
                name = mname.group(0)
                sid = sid or name2id.get(name, "")

        # æ™‚æ®µ
        text = L.replace("å¹´","/").replace("æœˆ","/").replace("æ—¥","")
        cand = re.findall(r"(\d{2,3}\s*/\s*\d{1,2}\s*/\s*\d{1,2}|\d{1,2}\s*/\s*\d{1,2})", text)
        if len(cand) >= 2:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            d2 = roc_to_date(cand[1], fallback_roc_year=fallback_roc_year)
        elif len(cand) == 1:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year); d2 = d1
        else:
            d1 = d2 = None

        leave_type = detect_leave_type(L, extra_type_keywords)

        if sid and d1 and d2:
            rows.append({"sid": sid, "name": name, "start": d1, "end": d2, "type": leave_type, "raw": L})

    return pd.DataFrame(rows).drop_duplicates()

def expand_leave_days(df_leave: pd.DataFrame) -> Dict[str, List[date]]:
    mp: Dict[str, List[date]] = {}
    if df_leave.empty: return mp
    for _, r in df_leave.iterrows():
        if not r["sid"] or pd.isna(r["start"]) or pd.isna(r["end"]): continue
        cur = r["start"]
        while cur <= r["end"]:
            mp.setdefault(r["sid"], []).append(cur)
            cur += timedelta(days=1)
    return mp

def build_five_checks(df_guard: pd.DataFrame, df_squad: pd.DataFrame, df_leave: pd.DataFrame) -> pd.DataFrame:
    """
    ä»¥ (æ—¥æœŸ Ã— å­¸è™Ÿ Ã— å§“å) ç‚ºåˆ—ï¼Œè¼¸å‡ºäº”æ¬„æª¢æ ¸ï¼š
      éšŠéƒ¨ç°½å‡º / éšŠéƒ¨ç°½å…¥ / ç´™æœ¬å‡å–® / å¤§é–€ç°½å‡º / å¤§é–€ç°½å…¥
    """
    keys = set()
    for df in [df_guard, df_squad]:
        if df.empty: continue
        for _, r in df.iterrows():
            if r.get("date") and r.get("sid"):
                keys.add((r["date"], r["sid"], r.get("name","")))

    # æœ‰å‡å–®ä¹Ÿè¦åˆ—å…¥äº”æ¬„è¦†æ ¸
    if not df_leave.empty:
        for _, r in df_leave.iterrows():
            d = r["start"]
            while d <= r["end"]:
                keys.add((date_to_roc(d), r["sid"], r.get("name","")))
                d += timedelta(days=1)

    rows = [{
        "æ—¥æœŸ(ROC)": droc,
        "å­¸è™Ÿ": sid,
        "å§“å": name,
        "éšŠéƒ¨ç°½å‡º": "X",
        "éšŠéƒ¨ç°½å…¥": "X",
        "ç´™æœ¬å‡å–®": "X",
        "å¤§é–€ç°½å‡º": "X",
        "å¤§é–€ç°½å…¥": "X",
    } for (droc, sid, name) in sorted(keys)]
    table = pd.DataFrame(rows)

    # è£œå§“å
    def lookup_name(sid):
        for df in (df_guard, df_squad):
            t = df[df["sid"]==sid]["name"]
            if not t.empty: return t.iloc[0]
        return ""
    if not table.empty:
        table["å§“å"] = table.apply(lambda r: r["å§“å"] or lookup_name(r["å­¸è™Ÿ"]), axis=1)

    # æ‰“å‹¾å‡ºå…¥ï¼ˆéšŠéƒ¨/å¤§é–€ï¼‰
    def mark(df, col_out, col_in):
        if df.empty: return
        for _, r in df.iterrows():
            m = (table["æ—¥æœŸ(ROC)"] == r["date"]) & (table["å­¸è™Ÿ"] == r["sid"])
            if r.get("dir") == "å‡º":
                table.loc[m, col_out] = "V"
            elif r.get("dir") == "å…¥":
                table.loc[m, col_in"] = "V"

    # ä¿®æ­£ï¼šå¼•è™Ÿå°ç¨±
    def mark(df, col_out, col_in):
        if df.empty: return
        for _, r in df.iterrows():
            m = (table["æ—¥æœŸ(ROC)"] == r["date"]) & (table["å­¸è™Ÿ"] == r["sid"])
            if r.get("dir") == "å‡º":
                table.loc[m, col_out] = "V"
            elif r.get("dir") == "å…¥":
                table.loc[m, col_in] = "V"

    mark(df_squad, "éšŠéƒ¨ç°½å‡º", "éšŠéƒ¨ç°½å…¥")
    mark(df_guard, "å¤§é–€ç°½å‡º", "å¤§é–€ç°½å…¥")

    # ç´™æœ¬å‡å–®è¦†æ ¸ï¼ˆç•¶å¤©åŒ…å«æ–¼å€é–“å³ Vï¼‰
    if not df_leave.empty and not table.empty:
        daymap = expand_leave_days(df_leave)  # sid -> [date...]
        for i, r in table.iterrows():
            sid = r["å­¸è™Ÿ"]; d_ = roc_to_date(r["æ—¥æœŸ(ROC)"])
            if sid in daymap and d_ and any(x == d_ for x in daymap[sid]):
                table.loc[i, "ç´™æœ¬å‡å–®"] = "V"

    return table.sort_values(["æ—¥æœŸ(ROC)", "å­¸è™Ÿ"])

def build_download_excel(dfs: Dict[str, pd.DataFrame]) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as w:
        for name, df in dfs.items():
            if df is None or df.empty: continue
            df.to_excel(w, index=False, sheet_name=name[:31] or "Sheet1")
    bio.seek(0)
    return bio.read()

# ========================
# Streamlit UI
# ========================
st.set_page_config(page_title="å·®å‡ç®¡ç†å“¡ Web v2.3.3 (Photo-only + å‡åˆ¥è‡ªè¨‚)", layout="wide")
st.title("ğŸ“‹ å·®å‡ç®¡ç†å“¡ï¼ˆWeb v2.3.3ï¼‰ï½œç›¸ç‰‡ï¼šç°½å‡ºå…¥ + ç´™æœ¬å‡å–® â†’ Excel å ±è¡¨ï¼ˆå«å‡åˆ¥è‡ªè¨‚ï¼‰")

with st.expander("â„¹ï¸ ä½¿ç”¨èªªæ˜", expanded=True):
    st.markdown("""
**åªä¸Šå‚³ç…§ç‰‡å³å¯å®Œæˆæª¢æ ¸èˆ‡åŒ¯å‡ºï¼š**
1. ä¸Šå‚³ `name_id_clean.txt`ï¼ˆå­¸è™Ÿ â†” å§“åï¼›é †åºä¸é™ï¼›ç©ºç™½/é€—è™Ÿ/Tab éƒ½å¯ï¼‰ã€‚
2. ä¸Šå‚³ **ç°½å‡ºå…¥ç…§ç‰‡**ï¼šå·¦ã€Œè­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ã€ã€å³ã€Œç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ã€ã€‚
   - è¾¨è­˜ **å‡º/å…¥æ‰“å‹¾**ï¼ˆâœ“/âœ”/V/å‹¾ï¼‰èˆ‡ **æ™‚é–“**ï¼ˆ09:05ã€9:5ã€9æ™‚05åˆ†ï¼‰ã€‚
3. ä¸Šå‚³ **ç´™æœ¬å‡å–®ç…§ç‰‡**ï¼šè‡ªå‹•è§£æ **å­¸è™Ÿ/å§“å** èˆ‡ **æ—¥æœŸå€é–“**ï¼ˆå–®æ—¥ä¹Ÿå¯ï¼‰ã€‚
   - å‡å–®åªå¯«å§“åæ²’å­¸è™Ÿ â†’ æœƒç”¨åå–®è‡ªå‹•è£œå­¸è™Ÿã€‚
4. å´æ¬„è¨­å®šã€Œ**é è¨­æ°‘åœ‹å¹´**ã€èˆ‡ã€Œ**è‡ªè¨‚å‡åˆ¥**ã€ã€‚
5. æŒ‰ã€Œ**OCR + è§£æ**ã€â†’ã€Œ**æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨**ã€â†’ ä¸‹è¼‰ Excelã€‚
""")

st.sidebar.header("æ—¥æœŸèˆ‡å‡åˆ¥è¨­å®š")
default_roc_year = st.sidebar.number_input("é è¨­æ°‘åœ‹å¹´ï¼ˆé‡åˆ°åªæœ‰ MM/DD æ™‚è£œå¹´ï¼‰", min_value=110, max_value=200, value=114)
st.sidebar.subheader("å‡åˆ¥é—œéµå­—ï¼ˆå¯è‡ªè¨‚ï¼‰")
custom_type_str = st.sidebar.text_input("é€—è™Ÿåˆ†éš”ï¼šå¦‚ å–ª,å©š,æ…°,è£œä¼‘,ç”¢,å…µå½¹", value="")
EXTRA_TYPE_KEYWORDS = [norm(x) for x in re.split(r"[,ï¼Œ\s]+", custom_type_str) if norm(x)]

# åå–®
st.header("ğŸ‘¥ ä¸Šå‚³å­¸è™Ÿå§“åå°ç…§è¡¨ï¼ˆTXTï¼‰")
mapping_file = st.file_uploader("name_id_clean.txt", type=["txt"])
id2name, name2id = {}, {}
if mapping_file:
    content = mapping_file.read().decode("utf-8")
    id2name, name2id = parse_mapping_txt(content)
    st.success(f"å·²è¼‰å…¥åå–®ï¼š{len(id2name)} ç­†")

# ç…§ç‰‡
st.header("ğŸ§¾ ä¸Šå‚³ç°½å‡ºå…¥ç…§ç‰‡")
col1, col2 = st.columns(2)
with col1:
    guard_imgs = st.file_uploader("è­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="guard")
with col2:
    squad_imgs = st.file_uploader("ç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="squad")

st.header("ğŸ“‘ ä¸Šå‚³ç´™æœ¬å‡å–®ç…§ç‰‡")
leave_imgs = st.file_uploader("å‡å–®ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="leave_imgs")

btn_ocr = st.button("ğŸ–‡ï¸ OCR + è§£æ")
btn_compare = st.button("ğŸ” æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨")

# session æš«å­˜
for key in ["df_guard","df_squad","df_leave","five_check"]:
    if key not in st.session_state:
        st.session_state[key] = pd.DataFrame()

# OCR + è§£æ
if btn_ocr:
    if not guard_imgs and not squad_imgs and not leave_imgs:
        st.warning("è«‹è‡³å°‘ä¸Šå‚³ä¸€å¼µç°½å‡ºå…¥æˆ–å‡å–®ç…§ç‰‡ã€‚")
    else:
        reader = load_ocr_reader()

        # å¤§é–€
        rows_g = []
        for f in (guard_imgs or []):
            lines = ocr_image(reader, f)
            df_g = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year,
                                    source_label="è­¦è¡›éšŠ", extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not df_g.empty: rows_g.append(df_g)
        st.session_state.df_guard = pd.concat(rows_g, ignore_index=True) if rows_g else pd.DataFrame()

        # éšŠéƒ¨
        rows_s = []
        for f in (squad_imgs or []):
            lines = ocr_image(reader, f)
            df_s = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year,
                                    source_label="ä¸­éšŠ", extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not df_s.empty: rows_s.append(df_s)
        st.session_state.df_squad = pd.concat(rows_s, ignore_index=True) if rows_s else pd.DataFrame()

        # å‡å–®ï¼ˆç…§ç‰‡ï¼‰
        rows_l = []
        for f in (leave_imgs or []):
            lines = ocr_image(reader, f)
            dfL = parse_leave_from_lines(lines, id2name, name2id,
                                         fallback_roc_year=default_roc_year,
                                         extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not dfL.empty: rows_l.append(dfL)
        st.session_state.df_leave = pd.concat(rows_l, ignore_index=True) if rows_l else pd.DataFrame()

        st.success(
            f"OCR å®Œæˆï¼šè­¦è¡›éšŠ {len(st.session_state.df_guard)} ç­†ï¼Œä¸­éšŠ {len(st.session_state.df_squad)} ç­†ï¼Œå‡å–® {len(st.session_state.df_leave)} ç­†ã€‚"
        )

# é¡¯ç¤º OCR çµæœ
if not st.session_state.df_guard.empty or not st.session_state.df_squad.empty:
    st.subheader("ğŸ” OCR çµæœï¼ˆç°½å‡ºå…¥ï¼Œå«æ™‚é–“èˆ‡å‡åˆ¥ï¼‰")
    df_view = pd.concat([st.session_state.df_guard, st.session_state.df_squad], ignore_index=True)
    st.dataframe(df_view, use_container_width=True)

if not st.session_state.df_leave.empty:
    st.subheader("ğŸ“„ å‡å–®ï¼ˆç…§ç‰‡ï¼‰è§£æçµæœï¼ˆå«å‡åˆ¥ï¼‰")
    st.dataframe(st.session_state.df_leave, use_container_width=True)

# æ¯”å° & åŒ¯å‡º
if btn_compare:
    if st.session_state.df_guard.empty and st.session_state.df_squad.empty and st.session_state.df_leave.empty:
        st.warning("è«‹å…ˆåŸ·è¡Œ OCRã€‚")
    else:
        five = build_five_checks(st.session_state.df_guard, st.session_state.df_squad, st.session_state.df_leave)
        st.session_state.five_check = five

        st.subheader("âœ… äº”æ¬„æª¢æ ¸è¡¨ï¼ˆV=æœ‰è¨˜éŒ„ / X=ç¼ºï¼‰")
        if not five.empty:
            st.dataframe(five, use_container_width=True)
        else:
            st.info("å°šç„¡å¯ç”¢å‡ºçš„äº”æ¬„æª¢æ ¸è³‡æ–™ã€‚")

        # åˆ†é–‹çµ±è¨ˆ
        if not five.empty:
            n1_out = int((five["éšŠéƒ¨ç°½å‡º"]=="X").sum())
            n1_in  = int((five["éšŠéƒ¨ç°½å…¥"]=="X").sum())
            n2_out = int((five["å¤§é–€ç°½å‡º"]=="X").sum())
            n2_in  = int((five["å¤§é–€ç°½å…¥"]=="X").sum())
            n3     = int((five["ç´™æœ¬å‡å–®"]=="X").sum())
        else:
            n1_out = n1_in = n2_out = n2_in = n3 = 0

        st.subheader("ğŸ“Š åˆ†é …çµ±è¨ˆ")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("æœªç°½å‡ºï¼ˆéšŠéƒ¨ï¼‰", n1_out)
        c2.metric("æœªç°½å…¥ï¼ˆéšŠéƒ¨ï¼‰", n1_in)
        c3.metric("æœªç°½å‡ºï¼ˆå¤§é–€ï¼‰", n2_out)
        c4.metric("æœªç°½å…¥ï¼ˆå¤§é–€ï¼‰", n2_in)
        c5.metric("æœªäº¤å‡å–®", n3)

        out_bytes = build_download_excel({
            "äº”æ¬„æª¢æ ¸": five,
            "è­¦è¡›éšŠ_OCR": st.session_state.df_guard,
            "ä¸­éšŠ_OCR": st.session_state.df_squad,
            "å‡å–®æ¸…å–®": st.session_state.df_leave
        })
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨ï¼ˆå«äº”æ¬„æª¢æ ¸ï¼‰",
            data=out_bytes,
            file_name=f"å·®å‡ç®¡ç†å“¡_äº”æ¬„æª¢æ ¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

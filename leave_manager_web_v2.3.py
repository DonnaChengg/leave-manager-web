# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
from typing import List, Dict, Tuple, Optional
from io import BytesIO
from PIL import Image
import re

# ========================
# OCR backend: EasyOCR
# ========================
@st.cache_resource(show_spinner=True)
def load_ocr_reader():
    import easyocr
    # ä¸­è‹±æ··æ’ï¼ŒGPU é—œé–‰ä»¥æ–¹ä¾¿é›²ç«¯/æœ¬æ©Ÿé€šç”¨
    return easyocr.Reader(['ch_sim', 'en'], gpu=False)

def ocr_image(reader, file) -> List[str]:
    """å›å‚³å½±åƒæ–‡å­—è¡Œï¼ˆä¿¡å¿ƒ >= 0.35ï¼‰ï¼Œåšå¸¸è¦‹æ­£è¦åŒ–ã€‚"""
    img = Image.open(file).convert("RGB")
    arr = np.array(img)
    res = reader.readtext(arr, detail=1)
    lines = [str(t[1]) for t in res if len(t) >= 3 and float(t[2]) >= 0.35]
    return [norm(x) for x in lines if norm(x)]

# ========================
# Utils
# ========================
FULLWIDTH_DIGITS = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
CHECK_MARKS = ["âœ“", "âœ”", "âœ…", "â˜‘", "â– ", "â–¡", "V", "v", "å‹¾", "âˆš"]

def norm(s: str) -> str:
    if s is None: return ""
    s = str(s).translate(FULLWIDTH_DIGITS)
    s = s.replace("\u3000", " ").replace("ï¼", "/")
    s = re.sub(r"[ \t]+", " ", s).strip()
    return s

def has_check(s: str) -> bool:
    return any(m in s for m in CHECK_MARKS)

def find_time(s: str) -> str:
    """
    æ“·å–æ™‚é–“å­—ä¸²ï¼š09:05ã€9:5ã€9ï¼š05ã€9æ™‚05åˆ† ç­‰
    å›å‚³æ¨™æº– HH:MMï¼ˆä¸è¶³è£œ 0ï¼‰ï¼Œæ‰¾ä¸åˆ°å›ç©ºå­—ä¸²ã€‚
    """
    x = s.replace("ï¼š", ":")
    # 9æ™‚05åˆ† / 9é»5åˆ†
    m = re.search(r"(\d{1,2})\s*[æ™‚ç‚¹é»]\s*(\d{1,2})\s*åˆ†?", x)
    if m:
        hh = int(m.group(1)); mm = int(m.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    # 09:05 / 9:5
    m2 = re.search(r"\b(\d{1,2})\s*:\s*(\d{1,2})\b", x)
    if m2:
        hh = int(m2.group(1)); mm = int(m2.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    return ""

def roc_to_date(roc_str: str, fallback_roc_year: Optional[int]=None) -> Optional[date]:
    """
    æ”¯æ´ï¼š
      - 114/10/31
      - 10/31 + fallback_roc_year
      - 114å¹´10æœˆ31æ—¥ / 10æœˆ31æ—¥
    """
    s = norm(roc_str).replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    s = re.sub(r"[.\-]", "/", s)

    m = re.search(r"(\d{2,3})\s*/\s*(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m:
        y, mo, da = map(int, m.groups())
        try:
            return date(y + 1911, mo, da)
        except Exception:
            return None

    m2 = re.search(r"(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m2 and fallback_roc_year:
        mo, da = map(int, m2.groups())
        try:
            return date(fallback_roc_year + 1911, mo, da)
        except Exception:
            return None

    return None

def date_to_roc(d: date) -> str:
    return f"{d.year - 1911}/{d.month:02d}/{d.day:02d}"

def parse_mapping_txt(txt: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    """name_id_clean.txt â†’ (id2name, name2id)"""
    id2name, name2id = {}, {}
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        parts = re.split(r"[\t,ï¼Œ\s]+", ln)
        if len(parts) < 2: continue
        a, b = parts[0], parts[1]
        if re.fullmatch(r"\d{7}", a) and not re.fullmatch(r"\d{7}", b):
            sid, name = a, b
        elif re.fullmatch(r"\d{7}", b) and not re.fullmatch(r"\d{7}", a):
            sid, name = b, a
        else:
            sid, name = (a, b) if re.search(r"\d", a) else (b, a)
        sid, name = norm(sid), norm(name)
        if re.fullmatch(r"\d{7}", sid) and name:
            id2name[sid] = name
            name2id[name] = sid
    return id2name, name2id

DIR_WORDS = {"å‡º":"å‡º", "å…¥":"å…¥"}
TYPE_WORDS = ["ç—…","äº‹","ç‰¹","å…¬","å‡","è¬›","æ›¸"]  # å¯åšå‡åˆ¥æç¤ºï¼Œä¸å½±éŸ¿äº”æ¬„

def parse_sign_lines(lines: List[str], id2name: Dict[str,str], fallback_roc_year: int,
                     source_label: str) -> pd.DataFrame:
    """
    å°‡ã€ŒéšŠéƒ¨ / å¤§é–€ã€ç…§ç‰‡ OCR è¡Œè½‰ç‚ºçµæ§‹è³‡æ–™ï¼š
    æ¬„ä½ï¼šsid, name, date(ROC), dir(å‡º/å…¥), time(HH:MM), type, source, raw
    """
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue

        sid = ""
        m_sid = re.search(r"(\d{7})", L)
        if m_sid: sid = m_sid.group(1)

        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname: name = mname.group(0)

        d = roc_to_date(L, fallback_roc_year=fallback_roc_year)
        time_ = find_time(L)

        dir_ = ""
        # å„ªå…ˆçœ‹æ˜¯å¦åŒä¸€è¡Œå¯«å‡º/å…¥
        for k in DIR_WORDS:
            if k in L:
                dir_ = DIR_WORDS[k]; break

        # è‹¥æ–‡å­—æ²’å‡ºç¾å‡º/å…¥ï¼Œä½†æœ‰æ‰“å‹¾ç¬¦è™Ÿï¼Œå˜—è©¦å¾ç›¸é„°è©åˆ¤å®šï¼ˆå¸¸è¦‹æ ¼å¼ï¼šâ–¡å‡º  â–¡å…¥ï¼‰
        if not dir_ and has_check(L):
            # ä¾‹ï¼šã€Œå‡º â–¡ å…¥ âœ“ã€æˆ–ã€Œâ–¡å‡º âœ“å…¥ã€
            # å˜—è©¦æŠ“ã€Œå‡ºã€æˆ–ã€Œå…¥ã€å­—å¾Œçš„å‹¾
            if re.search(r"å‡º.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å‡º"
            elif re.search(r"å…¥.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å…¥"

        kind = ""
        for k in TYPE_WORDS:
            if k in L:
                kind = k; break

        if any([sid, name, d, dir_, time_]) or has_check(L):
            rows.append({
                "sid": sid,
                "name": name,
                "date": date_to_roc(d) if d else "",
                "dir": dir_,
                "time": time_,
                "type": kind,
                "source": source_label,
                "raw": L
            })

    df = pd.DataFrame(rows, columns=["sid","name","date","dir","time","type","source","raw"]).drop_duplicates()
    if not df.empty:
        df["name"] = df.apply(lambda r: id2name.get(r["sid"], r["name"]), axis=1)
    return df

def load_leave_excel(file, fallback_roc_year: int) -> pd.DataFrame:
    """
    è®€å–ç´™æœ¬å‡å–®ï¼ˆExcel/CSVï¼‰
    å¿…è¦æ¬„ä½ï¼ˆå¤§å°å¯«/ä¸­è‹±ç„¡æ‰€è¬‚ï¼‰ï¼šsid/å­¸è™Ÿã€start/é–‹å§‹ã€end/çµæŸ
    æ”¯æ´ ROC æˆ–è¥¿å…ƒæ—¥æœŸï¼›è‹¥åªæœ‰ MM/DD æœƒç”¨ fallback_roc_year è£œå¹´
    """
    fn = file.name.lower()
    if fn.endswith(".csv"):
        df = pd.read_csv(file)
    else:
        df = pd.read_excel(file)

    cols = {c.lower(): c for c in df.columns}
    def pick(*names):
        for n in names:
            if n in cols: return cols[n]
        return None

    c_sid = pick("sid", "å­¸è™Ÿ", "id")
    c_name = pick("name", "å§“å")
    c_start = pick("start", "é–‹å§‹", "é–‹å§‹æ—¥", "å¾", "from")
    c_end = pick("end", "çµæŸ", "çµæŸæ—¥", "è‡³", "to")
    if not (c_sid and c_start and c_end):
        raise ValueError("å‡å–®éœ€åŒ…å«æ¬„ä½ï¼šsid/å­¸è™Ÿã€start/é–‹å§‹ã€end/çµæŸã€‚")

    def parse_d(x):
        if pd.isna(x): return None
        if isinstance(x, (datetime, date)):
            return x.date() if isinstance(x, datetime) else x
        s = str(x)
        d = roc_to_date(s, fallback_roc_year=fallback_roc_year)
        if d: return d
        try:
            return pd.to_datetime(s).date()
        except Exception:
            return None

    out = pd.DataFrame({
        "sid": df[c_sid].astype(str).str.extract(r"(\d{7})", expand=False),
        "name": df[c_name] if c_name else "",
        "start": df[c_start].apply(parse_d),
        "end": df[c_end].apply(parse_d),
    }).dropna(subset=["sid","start","end"])
    return out

def parse_leave_from_lines(lines: List[str], fallback_roc_year: int) -> pd.DataFrame:
    """
    å¾å‡å–®ç…§ç‰‡ OCR è¡Œå»ºç«‹ã€Œç´™æœ¬å‡å–®å€é–“ã€
    æ”¯æ´ï¼š
      114/10/27 ~ 114/10/29
      10/27 ~ 10/29ï¼ˆè£œå¹´ï¼‰
      åªæœ‰å–®æ—¥ â†’ start=end
    """
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue
        sid = ""
        m = re.search(r"(\d{7})", L)
        if m: sid = m.group(1)

        # æ“·å– 1~2 å€‹æ—¥æœŸå­—ä¸²
        text = L.replace("å¹´","/").replace("æœˆ","/").replace("æ—¥","")
        cand = re.findall(r"(\d{2,3}\s*/\s*\d{1,2}\s*/\s*\d{1,2}|\d{1,2}\s*/\s*\d{1,2})", text)
        if len(cand) >= 2:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            d2 = roc_to_date(cand[1], fallback_roc_year=fallback_roc_year)
            if sid and d1 and d2:
                rows.append({"sid": sid, "name": "", "start": d1, "end": d2})
        elif len(cand) == 1:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            if sid and d1:
                rows.append({"sid": sid, "name": "", "start": d1, "end": d1})

    return pd.DataFrame(rows).drop_duplicates()

def expand_leave_days(df_leave: pd.DataFrame) -> Dict[str, List[date]]:
    """
    æŠŠæ¯ä¸€ç­† sid çš„ (start~end) å±•æˆæ¯æ—¥æ¸…å–®ï¼Œå›å‚³ sid -> [date, ...]
    """
    mp: Dict[str, List[date]] = {}
    if df_leave.empty: return mp
    for _, r in df_leave.iterrows():
        if not r["sid"] or pd.isna(r["start"]) or pd.isna(r["end"]): continue
        cur = r["start"]
        while cur <= r["end"]:
            mp.setdefault(r["sid"], []).append(cur)
            cur += timedelta(days=1)
    return mp

def build_five_checks(df_guard: pd.DataFrame, df_squad: pd.DataFrame, df_leave: pd.DataFrame) -> pd.DataFrame:
    """
    ä»¥ (æ—¥æœŸ Ã— å­¸è™Ÿ Ã— å§“å) ç‚ºåˆ—ï¼Œè¼¸å‡ºäº”æ¬„æª¢æ ¸ï¼š
      éšŠéƒ¨ç°½å‡º / éšŠéƒ¨ç°½å…¥ / ç´™æœ¬å‡å–® / å¤§é–€ç°½å‡º / å¤§é–€ç°½å…¥
    """
    keys = set()
    for df in [df_guard, df_squad]:
        if df.empty: continue
        for _, r in df.iterrows():
            if r.get("date") and r.get("sid"):
                keys.add((r["date"], r["sid"], r.get("name","")))

    # æœ‰å‡å–®ä½†æ²’æœ‰ä»»ä½•å‡ºå…¥ç´€éŒ„ï¼Œä¹Ÿè¦åˆ—å…¥äº”æ¬„ä»¥åˆ©è¦†æ ¸
    if not df_leave.empty:
        for _, r in df_leave.iterrows():
            d = r["start"]
            while d <= r["end"]:
                keys.add((date_to_roc(d), r["sid"], r.get("name","")))
                d += timedelta(days=1)

    rows = [{
        "æ—¥æœŸ(ROC)": droc,
        "å­¸è™Ÿ": sid,
        "å§“å": name,
        "éšŠéƒ¨ç°½å‡º": "X",
        "éšŠéƒ¨ç°½å…¥": "X",
        "ç´™æœ¬å‡å–®": "X",
        "å¤§é–€ç°½å‡º": "X",
        "å¤§é–€ç°½å…¥": "X",
    } for (droc, sid, name) in sorted(keys)]
    table = pd.DataFrame(rows)

    # è£œå§“å
    def lookup_name(sid):
        for df in (df_guard, df_squad):
            t = df[df["sid"]==sid]["name"]
            if not t.empty: return t.iloc[0]
        return ""
    if not table.empty:
        table["å§“å"] = table.apply(lambda r: r["å§“å"] or lookup_name(r["å­¸è™Ÿ"]), axis=1)

    # æ‰“å‹¾ç°½å‡º / ç°½å…¥ï¼ˆéšŠéƒ¨ / å¤§é–€åˆ†é–‹ï¼‰
    def mark(df, col_out, col_in):
        if df.empty: return
        for _, r in df.iterrows():
            key = (r["date"], r["sid"])
            m = (table["æ—¥æœŸ(ROC)"] == key[0]) & (table["å­¸è™Ÿ"] == key[1])
            if r.get("dir") == "å‡º":
                table.loc[m, col_out] = "V"
            elif r.get("dir") == "å…¥":
                table.loc[m, col_in] = "V"

    mark(df_squad, "éšŠéƒ¨ç°½å‡º", "éšŠéƒ¨ç°½å…¥")
    mark(df_guard, "å¤§é–€ç°½å‡º", "å¤§é–€ç°½å…¥")

    # ç´™æœ¬å‡å–®è¦†æ ¸ï¼ˆç•¶å¤©æœ‰åŒ…å«æ–¼å€é–“å³ Vï¼‰
    if not df_leave.empty and not table.empty:
        daymap = expand_leave_days(df_leave)  # sid -> [date...]
        for i, r in table.iterrows():
            sid = r["å­¸è™Ÿ"]
            d_ = roc_to_date(r["æ—¥æœŸ(ROC)"])
            if sid in daymap and d_ and any(x == d_ for x in daymap[sid]):
                table.loc[i, "ç´™æœ¬å‡å–®"] = "V"

    return table.sort_values(["æ—¥æœŸ(ROC)", "å­¸è™Ÿ"])

def build_download_excel(dfs: Dict[str, pd.DataFrame]) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as w:
        for name, df in dfs.items():
            if df is None or df.empty: continue
            df.to_excel(w, index=False, sheet_name=name[:31] or "Sheet1")
    bio.seek(0)
    return bio.read()

# ========================
# Streamlit UI
# ========================
st.set_page_config(page_title="å·®å‡ç®¡ç†å“¡ Web v2.3", layout="wide")
st.title("ğŸ“‹ å·®å‡ç®¡ç†å“¡ï¼ˆWeb v2.3ï¼‰ï½œOCR + åå–®/å‡å–®è¦†æ ¸ + éšŠéƒ¨/å¤§é–€å‡ºå…¥ + åˆ†é …çµ±è¨ˆ + åŒ¯å‡º")

with st.expander("â„¹ï¸ ä½¿ç”¨èªªæ˜", expanded=True):
    st.markdown("""
**æµç¨‹ï¼š**
1. ä¸Šå‚³ `name_id_clean.txt`ï¼ˆå­¸è™Ÿ â†” å§“åï¼Œé †åºä¸é™ï¼›ç©ºç™½/é€—è™Ÿ/Tab çš†å¯ï¼‰ã€‚
2. ä¸Šå‚³ **ç°½å‡ºå…¥ç…§ç‰‡**ï¼šå·¦é‚Šã€Œè­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ã€ã€å³é‚Šã€Œç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ã€ã€‚
   - æ”¯æ´ç…§ç‰‡ä¸Š**æ‰“å‹¾**ï¼ˆâœ“/âœ”/V/å‹¾ï¼‰åŠ**æ™‚é–“**ï¼ˆ09:05ã€9:5ã€9æ™‚05åˆ†ï¼‰ã€‚
3. ä¸Šå‚³ **ç´™æœ¬å‡å–®**ï¼ˆæ“‡ä¸€ï¼‰ï¼š
   - Excel/CSVï¼ˆæœ‰ sid/start/endï¼‰æˆ– å‡å–®ç…§ç‰‡ï¼ˆæœƒè§£ææ—¥æœŸå€é–“ï¼‰ã€‚
4. å´æ¬„è¨­å®šã€Œ**é è¨­æ°‘åœ‹å¹´**ã€ï¼ˆç•¶ OCR åªæœ‰ MM/DD æ™‚ç”¨æ­¤è£œå¹´ï¼‰ã€‚
5. é»ã€Œ**OCR + è§£æ**ã€â†’ã€Œ**æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨**ã€â†’ å¯ä¸‹è¼‰ Excelã€‚
""")

st.sidebar.header("æ—¥æœŸè¨­å®š")
default_roc_year = st.sidebar.number_input("é è¨­æ°‘åœ‹å¹´ï¼ˆè¡¨å–®åªæœ‰ MM/DD æ™‚ä½¿ç”¨ï¼‰", min_value=110, max_value=200, value=114)

# åå–®
st.header("ğŸ‘¥ ä¸Šå‚³å­¸è™Ÿå§“åå°ç…§è¡¨ï¼ˆTXTï¼‰")
mapping_file = st.file_uploader("name_id_clean.txt", type=["txt"])
id2name, name2id = {}, {}
if mapping_file:
    content = mapping_file.read().decode("utf-8")
    id2name, name2id = parse_mapping_txt(content)
    st.success(f"å·²è¼‰å…¥åå–®ï¼š{len(id2name)} ç­†")

# ç…§ç‰‡ï¼ˆå¤§é–€/éšŠéƒ¨ï¼‰
st.header("ğŸ§¾ ä¸Šå‚³ç°½å‡ºå…¥ç…§ç‰‡ï¼ˆæœƒè¾¨è­˜å‡º/å…¥æ‰“å‹¾èˆ‡æ™‚é–“ï¼‰")
col1, col2 = st.columns(2)
with col1:
    guard_imgs = st.file_uploader("è­¦è¡›éšŠï¼ˆå­¸ç”Ÿç¸½éšŠ/å¤§é–€ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="guard")
with col2:
    squad_imgs = st.file_uploader("ç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="squad")

# å‡å–®ï¼ˆExcel/CSV æˆ– ç…§ç‰‡ï¼‰
st.header("ğŸ“‘ ä¸Šå‚³ç´™æœ¬å‡å–®ï¼ˆExcel/CSV æˆ– å‡å–®ç…§ç‰‡ï¼‰")
leave_file = st.file_uploader("Excel/CSVï¼ˆéœ€å«ï¼šsid/å­¸è™Ÿã€start/é–‹å§‹ã€end/çµæŸï¼‰", type=["xlsx","xls","csv"])
leave_imgs = st.file_uploader("å‡å–®ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="leave_imgs")

# æŒ‰éˆ•
btn_ocr = st.button("ğŸ–‡ï¸ OCR + è§£æ")
btn_compare = st.button("ğŸ” æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨")

# session æš«å­˜
for key in ["df_guard","df_squad","df_leave_from_imgs","df_leave","five_check"]:
    if key not in st.session_state:
        st.session_state[key] = pd.DataFrame()

# OCR + è§£æ
if btn_ocr:
    if not guard_imgs and not squad_imgs and not leave_imgs:
        st.warning("è«‹è‡³å°‘ä¸Šå‚³ä¸€å¼µç°½å‡ºå…¥æˆ–å‡å–®ç…§ç‰‡ã€‚")
    else:
        reader = load_ocr_reader()

        # å¤§é–€ï¼ˆè­¦è¡›éšŠï¼‰
        rows_g = []
        for f in (guard_imgs or []):
            lines = ocr_image(reader, f)
            df_g = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year, source_label="è­¦è¡›éšŠ")
            if not df_g.empty: rows_g.append(df_g)
        st.session_state.df_guard = pd.concat(rows_g, ignore_index=True) if rows_g else pd.DataFrame()

        # éšŠéƒ¨ï¼ˆä¸­éšŠï¼‰
        rows_s = []
        for f in (squad_imgs or []):
            lines = ocr_image(reader, f)
            df_s = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year, source_label="ä¸­éšŠ")
            if not df_s.empty: rows_s.append(df_s)
        st.session_state.df_squad = pd.concat(rows_s, ignore_index=True) if rows_s else pd.DataFrame()

        # å‡å–®ç…§ç‰‡ â†’ æ—¥æœŸå€é–“
        rows_l = []
        for f in (leave_imgs or []):
            lines = ocr_image(reader, f)
            dfL = parse_leave_from_lines(lines, fallback_roc_year=default_roc_year)
            if not dfL.empty: rows_l.append(dfL)
        st.session_state.df_leave_from_imgs = pd.concat(rows_l, ignore_index=True) if rows_l else pd.DataFrame()

        st.success(
            f"OCR å®Œæˆï¼šè­¦è¡›éšŠ {len(st.session_state.df_guard)} ç­†ï¼Œä¸­éšŠ {len(st.session_state.df_squad)} ç­†ï¼Œå‡å–®(ç…§ç‰‡) {len(st.session_state.df_leave_from_imgs)} ç­†ã€‚"
        )

# é¡¯ç¤º OCR çµæœï¼ˆå«æ™‚é–“ï¼‰
if not st.session_state.df_guard.empty or not st.session_state.df_squad.empty:
    st.subheader("ğŸ” OCR çµæœï¼ˆç°½å‡ºå…¥ï¼Œå«æ™‚é–“ï¼‰")
    df_view = pd.concat([st.session_state.df_guard, st.session_state.df_squad], ignore_index=True)
    st.dataframe(df_view, use_container_width=True)

# è®€å‡å–® Excel/CSVï¼ˆè‹¥æœ‰ï¼‰
if leave_file:
    try:
        st.session_state.df_leave = load_leave_excel(leave_file, fallback_roc_year=default_roc_year)
        st.success(f"å·²è¼‰å…¥å‡å–®ï¼ˆExcel/CSVï¼‰ï¼š{len(st.session_state.df_leave)} ç­†")
    except Exception as e:
        st.error(f"å‡å–®è®€å–å¤±æ•—ï¼š{e}")

# æ¯”å° & è¼¸å‡º
if btn_compare:
    if st.session_state.df_guard.empty and st.session_state.df_squad.empty and st.session_state.df_leave.empty and st.session_state.df_leave_from_imgs.empty:
        st.warning("è«‹å…ˆåŸ·è¡Œ OCR æˆ–ä¸Šå‚³å‡å–® Excelã€‚")
    else:
        # å‡å–®ä¾†æºï¼šExcel å„ªå…ˆï¼Œç„¡å‰‡ç”¨ OCR
        df_leave_final = st.session_state.df_leave.copy() if not st.session_state.df_leave.empty \
            else st.session_state.df_leave_from_imgs.copy()

        # äº”æ¬„æª¢æ ¸
        five = build_five_checks(st.session_state.df_guard, st.session_state.df_squad, df_leave_final)
        st.session_state.five_check = five

        st.subheader("âœ… äº”æ¬„æª¢æ ¸è¡¨ï¼ˆV=æœ‰è¨˜éŒ„ / X=ç¼ºï¼‰")
        if not five.empty:
            st.dataframe(five, use_container_width=True)
        else:
            st.info("å°šç„¡å¯ç”¢å‡ºçš„äº”æ¬„æª¢æ ¸è³‡æ–™ã€‚")

        # åˆ†é …çµ±è¨ˆï¼ˆå‡º/å…¥åˆ†é–‹ï¼‰
        if not five.empty:
            n1_out = int((five["éšŠéƒ¨ç°½å‡º"]=="X").sum())
            n1_in  = int((five["éšŠéƒ¨ç°½å…¥"]=="X").sum())
            n2_out = int((five["å¤§é–€ç°½å‡º"]=="X").sum())
            n2_in  = int((five["å¤§é–€ç°½å…¥"]=="X").sum())
            n3     = int((five["ç´™æœ¬å‡å–®"]=="X").sum())
        else:
            n1_out = n1_in = n2_out = n2_in = n3 = 0

        st.subheader("ğŸ“Š åˆ†é …çµ±è¨ˆ")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("æœªç°½å‡ºï¼ˆéšŠéƒ¨ï¼‰", n1_out)
        c2.metric("æœªç°½å…¥ï¼ˆéšŠéƒ¨ï¼‰", n1_in)
        c3.metric("æœªç°½å‡ºï¼ˆå¤§é–€ï¼‰", n2_out)
        c4.metric("æœªç°½å…¥ï¼ˆå¤§é–€ï¼‰", n2_in)
        c5.metric("æœªäº¤å‡å–®", n3)

        # åŒ¯å‡º
        out_bytes = build_download_excel({
            "äº”æ¬„æª¢æ ¸": five,
            "è­¦è¡›éšŠ_OCR": st.session_state.df_guard,
            "ä¸­éšŠ_OCR": st.session_state.df_squad,
            "å‡å–®æ¸…å–®": df_leave_final
        })
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨ï¼ˆå«äº”æ¬„æª¢æ ¸ï¼‰",
            data=out_bytes,
            file_name=f"å·®å‡ç®¡ç†å“¡_äº”æ¬„æª¢æ ¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

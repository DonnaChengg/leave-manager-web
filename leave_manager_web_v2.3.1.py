# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
from typing import List, Dict, Tuple, Optional
from io import BytesIO
from PIL import Image
import re

# ========================
# OCR backend: RapidOCR (ONNXRuntime)
# ========================
@st.cache_resource(show_spinner=True)
def load_ocr_reader():
    from rapidocr_onnxruntime import RapidOCR
    # use_cuda=False (CPU), text detection+recognitionï¼Œé è¨­ä¸­æ–‡å¯
    return RapidOCR()

def ocr_image(reader, file) -> List[str]:
    """å›å‚³å½±åƒæ–‡å­—è¡Œï¼ˆä¿¡å¿ƒ >= 0.35ï¼‰ï¼Œåšå¸¸è¦‹æ­£è¦åŒ–ã€‚"""
    img = Image.open(file).convert("RGB")
    arr = np.array(img)
    try:
        result, _ = reader(arr)   # result: list of [box, text, score]
    except Exception:
        result = []
    lines = []
    for item in (result or []):
        if not item or len(item) < 3:
            continue
        text = str(item[1])
        score = float(item[2]) if item[2] is not None else 0.0
        if score >= 0.35:
            text = norm(text)
            if text:
                lines.append(text)
    return lines

# ========================
# Utils
# ========================
FULLWIDTH_DIGITS = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
CHECK_MARKS = ["âœ“", "âœ”", "âœ…", "â˜‘", "â– ", "â–¡", "V", "v", "å‹¾", "âˆš"]

def norm(s: str) -> str:
    if s is None: return ""
    s = str(s).translate(FULLWIDTH_DIGITS)
    s = s.replace("\u3000", " ").replace("ï¼", "/")
    s = re.sub(r"[ \t]+", " ", s).strip()
    return s

def has_check(s: str) -> bool:
    return any(m in s for m in CHECK_MARKS)

def find_time(s: str) -> str:
    x = s.replace("ï¼š", ":")
    m = re.search(r"(\d{1,2})\s*[æ™‚ç‚¹é»]\s*(\d{1,2})\s*åˆ†?", x)
    if m:
        hh = int(m.group(1)); mm = int(m.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    m2 = re.search(r"\b(\d{1,2})\s*:\s*(\d{1,2})\b", x)
    if m2:
        hh = int(m2.group(1)); mm = int(m2.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    return ""

def roc_to_date(roc_str: str, fallback_roc_year: Optional[int]=None) -> Optional[date]:
    s = norm(roc_str).replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    s = re.sub(r"[.\-]", "/", s)
    m = re.search(r"(\d{2,3})\s*/\s*(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m:
        y, mo, da = map(int, m.groups())
        try:
            return date(y + 1911, mo, da)
        except Exception:
            return None
    m2 = re.search(r"(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m2 and fallback_roc_year:
        mo, da = map(int, m2.groups())
        try:
            return date(fallback_roc_year + 1911, mo, da)
        except Exception:
            return None
    return None

def date_to_roc(d: date) -> str:
    return f"{d.year - 1911}/{d.month:02d}/{d.day:02d}"

def parse_mapping_txt(txt: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    id2name, name2id = {}, {}
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        parts = re.split(r"[\t,ï¼Œ\s]+", ln)
        if len(parts) < 2: continue
        a, b = parts[0], parts[1]
        if re.fullmatch(r"\d{7}", a) and not re.fullmatch(r"\d{7}", b):
            sid, name = a, b
        elif re.fullmatch(r"\d{7}", b) and not re.fullmatch(r"\d{7}", a):
            sid, name = b, a
        else:
            sid, name = (a, b) if re.search(r"\d", a) else (b, a)
        sid, name = norm(sid), norm(name)
        if re.fullmatch(r"\d{7}", sid) and name:
            id2name[sid] = name
            name2id[name] = sid
    return id2name, name2id

DIR_WORDS = {"å‡º":"å‡º", "å…¥":"å…¥"}
TYPE_WORDS = ["ç—…","äº‹","ç‰¹","å…¬","å‡","è¬›","æ›¸"]

def parse_sign_lines(lines: List[str], id2name: Dict[str,str], fallback_roc_year: int,
                     source_label: str) -> pd.DataFrame:
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue
        sid = ""
        m_sid = re.search(r"(\d{7})", L)
        if m_sid: sid = m_sid.group(1)
        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname: name = mname.group(0)
        d = roc_to_date(L, fallback_roc_year=fallback_roc_year)
        time_ = find_time(L)
        dir_ = ""
        for k in DIR_WORDS:
            if k in L:
                dir_ = DIR_WORDS[k]; break
        if not dir_ and has_check(L):
            if re.search(r"å‡º.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å‡º"
            elif re.search(r"å…¥.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å…¥"
        kind = ""
        for k in TYPE_WORDS:
            if k in L:
                kind = k; break
        if any([sid, name, d, dir_, time_]) or has_check(L):
            rows.append({
                "sid": sid,
                "name": name,
                "date": date_to_roc(d) if d else "",
                "dir": dir_,
                "time": time_,
                "type": kind,
                "source": source_label,
                "raw": L
            })
    df = pd.DataFrame(rows, columns=["sid","name","date","dir","time","type","source","raw"]).drop_duplicates()
    if not df.empty:
        df["name"] = df.apply(lambda r: id2name.get(r["sid"], r["name"]), axis=1)
    return df

def load_leave_excel(file, fallback_roc_year: int) -> pd.DataFrame:
    fn = file.name.lower()
    if fn.endswith(".csv"):
        df = pd.read_csv(file)
    else:
        df = pd.read_excel(file)
    cols = {c.lower(): c for c in df.columns}
    def pick(*names):
        for n in names:
            if n in cols: return cols[n]
        return None
    c_sid = pick("sid", "å­¸è™Ÿ", "id")
    c_name = pick("name", "å§“å")
    c_start = pick("start", "é–‹å§‹", "é–‹å§‹æ—¥", "å¾", "from")
    c_end = pick("end", "çµæŸ", "çµæŸæ—¥", "è‡³", "to")
    if not (c_sid and c_start and c_end):
        raise ValueError("å‡å–®éœ€åŒ…å«æ¬„ä½ï¼šsid/å­¸è™Ÿã€start/é–‹å§‹ã€end/çµæŸã€‚")
    def parse_d(x):
        if pd.isna(x): return None
        if isinstance(x, (datetime, date)):
            return x.date() if isinstance(x, datetime) else x
        s = str(x)
        d = roc_to_date(s, fallback_roc_year=fallback_roc_year)
        if d: return d
        try:
            return pd.to_datetime(s).date()
        except Exception:
            return None
    out = pd.DataFrame({
        "sid": df[c_sid].astype(str).str.extract(r"(\d{7})", expand=False),
        "name": df[c_name] if c_name else "",
        "start": df[c_start].apply(parse_d),
        "end": df[c_end].apply(parse_d),
    }).dropna(subset=["sid","start","end"])
    return out

def parse_leave_from_lines(lines: List[str], fallback_roc_year: int) -> pd.DataFrame:
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue
        sid = ""
        m = re.search(r"(\d{7})", L)
        if m: sid = m.group(1)
        text = L.replace("å¹´","/").replace("æœˆ","/").replace("æ—¥","")
        cand = re.findall(r"(\d{2,3}\s*/\s*\d{1,2}\s*/\s*\d{1,2}|\d{1,2}\s*/\s*\d{1,2})", text)
        if len(cand) >= 2:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            d2 = roc_to_date(cand[1], fallback_roc_year=fallback_roc_year)
            if sid and d1 and d2:
                rows.append({"sid": sid, "name": "", "start": d1, "end": d2})
        elif len(cand) == 1:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            if sid and d1:
                rows.append({"sid": sid, "name": "", "start": d1, "end": d1})
    return pd.DataFrame(rows).drop_duplicates()

def expand_leave_days(df_leave: pd.DataFrame) -> Dict[str, List[date]]:
    mp: Dict[str, List[date]] = {}
    if df_leave.empty: return mp
    for _, r in df_leave.iterrows():
        if not r["sid"] or pd.isna(r["start"]) or pd.isna(r["end"]): continue
        cur = r["start"]
        while cur <= r["end"]:
            mp.setdefault(r["sid"], []).append(cur)
            cur += timedelta(days=1)
    return mp

def build_five_checks(df_guard: pd.DataFrame, df_squad: pd.DataFrame, df_leave: pd.DataFrame) -> pd.DataFrame:
    keys = set()
    for df in [df_guard, df_squad]:
        if df.empty: continue
        for _, r in df.iterrows():
            if r.get("date") and r.get("sid"):
                keys.add((r["date"], r["sid"], r.get("name","")))
    if not df_leave.empty:
        for _, r in df_leave.iterrows():
            d = r["start"]
            while d <= r["end"]:
                keys.add((date_to_roc(d), r["sid"], r.get("name","")))
                d += timedelta(days=1)
    rows = [{
        "æ—¥æœŸ(ROC)": droc,
        "å­¸è™Ÿ": sid,
        "å§“å": name,
        "éšŠéƒ¨ç°½å‡º": "X",
        "éšŠéƒ¨ç°½å…¥": "X",
        "ç´™æœ¬å‡å–®": "X",
        "å¤§é–€ç°½å‡º": "X",
        "å¤§é–€ç°½å…¥": "X",
    } for (droc, sid, name) in sorted(keys)]
    table = pd.DataFrame(rows)
    def lookup_name(sid):
        for df in (df_guard, df_squad):
            t = df[df["sid"]==sid]["name"]
            if not t.empty: return t.iloc[0]
        return ""
    if not table.empty:
        table["å§“å"] = table.apply(lambda r: r["å§“å"] or lookup_name(r["å­¸è™Ÿ"]), axis=1)
    def mark(df, col_out, col_in):
        if df.empty: return
        for _, r in df.iterrows():
            m = (table["æ—¥æœŸ(ROC)"] == r["date"]) & (table["å­¸è™Ÿ"] == r["sid"])
            if r.get("dir") == "å‡º":
                table.loc[m, col_out] = "V"
            elif r.get("dir") == "å…¥":
                table.loc[m, col_in] = "V"
    mark(df_squad, "éšŠéƒ¨ç°½å‡º", "éšŠéƒ¨ç°½å…¥")
    mark(df_guard, "å¤§é–€ç°½å‡º", "å¤§é–€ç°½å…¥")
    if not df_leave.empty and not table.empty:
        daymap = expand_leave_days(df_leave)
        for i, r in table.iterrows():
            sid = r["å­¸è™Ÿ"]; d_ = roc_to_date(r["æ—¥æœŸ(ROC)"])
            if sid in daymap and d_ and any(x == d_ for x in daymap[sid]):
                table.loc[i, "ç´™æœ¬å‡å–®"] = "V"
    return table.sort_values(["æ—¥æœŸ(ROC)", "å­¸è™Ÿ"])

def build_download_excel(dfs: Dict[str, pd.DataFrame]) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as w:
        for name, df in dfs.items():
            if df is None or df.empty: continue
            df.to_excel(w, index=False, sheet_name=name[:31] or "Sheet1")
    bio.seek(0)
    return bio.read()

# ========================
# Streamlit UI
# ========================
st.set_page_config(page_title="å·®å‡ç®¡ç†å“¡ Web v2.3.1 (RapidOCR)", layout="wide")
st.title("ğŸ“‹ å·®å‡ç®¡ç†å“¡ï¼ˆWeb v2.3.1ï¼‰ï½œRapidOCR + åå–®/å‡å–®è¦†æ ¸ + éšŠéƒ¨/å¤§é–€å‡ºå…¥ + åˆ†é …çµ±è¨ˆ + åŒ¯å‡º")

with st.expander("â„¹ï¸ ä½¿ç”¨èªªæ˜", expanded=True):
    st.markdown("""
**æµç¨‹ï¼š**
1. ä¸Šå‚³ `name_id_clean.txt`ï¼ˆå­¸è™Ÿ â†” å§“åï¼Œç©ºç™½/é€—è™Ÿ/Tab çš†å¯ï¼‰ã€‚
2. ä¸Šå‚³ **ç°½å‡ºå…¥ç…§ç‰‡**ï¼šå·¦ã€Œè­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ã€ã€å³ã€Œç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ã€ã€‚
   - æ”¯æ´ç…§ç‰‡ä¸Šçš„**å‡º/å…¥æ‰“å‹¾**èˆ‡**æ™‚é–“**ï¼ˆ09:05ã€9:5ã€9æ™‚05åˆ†ï¼‰ã€‚
3. ä¸Šå‚³ **ç´™æœ¬å‡å–®**ï¼ˆæ“‡ä¸€ï¼‰ï¼šExcel/CSVï¼ˆæœ‰ sid/start/endï¼‰æˆ–å‡å–®ç…§ç‰‡ï¼ˆæœƒè§£æå€é–“ï¼‰ã€‚
4. å´æ¬„è¨­å®šã€Œ**é è¨­æ°‘åœ‹å¹´**ã€ï¼ˆç•¶ OCR åªæœ‰ MM/DD æ™‚ç”¨æ­¤è£œå¹´ï¼‰ã€‚
5. é»ã€Œ**OCR + è§£æ**ã€â†’ã€Œ**æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨**ã€â†’ ä¸‹è¼‰ Excelã€‚
""")

st.sidebar.header("æ—¥æœŸè¨­å®š")
default_roc_year = st.sidebar.number_input("é è¨­æ°‘åœ‹å¹´ï¼ˆè¡¨å–®åªæœ‰ MM/DD æ™‚ä½¿ç”¨ï¼‰", min_value=110, max_value=200, value=114)

# åå–®
st.header("ğŸ‘¥ ä¸Šå‚³å­¸è™Ÿå§“åå°ç…§è¡¨ï¼ˆTXTï¼‰")
mapping_file = st.file_uploader("name_id_clean.txt", type=["txt"])
id2name, name2id = {}, {}
if mapping_file:
    content = mapping_file.read().decode("utf-8")
    id2name, name2id = parse_mapping_txt(content)
    st.success(f"å·²è¼‰å…¥åå–®ï¼š{len(id2name)} ç­†")

# ç…§ç‰‡ï¼ˆå¤§é–€/éšŠéƒ¨ï¼‰
st.header("ğŸ§¾ ä¸Šå‚³ç°½å‡ºå…¥ç…§ç‰‡ï¼ˆè¾¨è­˜å‡º/å…¥æ‰“å‹¾èˆ‡æ™‚é–“ï¼‰")
col1, col2 = st.columns(2)
with col1:
    guard_imgs = st.file_uploader("è­¦è¡›éšŠï¼ˆå­¸ç”Ÿç¸½éšŠ/å¤§é–€ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="guard")
with col2:
    squad_imgs = st.file_uploader("ç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="squad")

# å‡å–®ï¼ˆExcel/CSV æˆ– ç…§ç‰‡ï¼‰
st.header("ğŸ“‘ ä¸Šå‚³ç´™æœ¬å‡å–®ï¼ˆExcel/CSV æˆ– å‡å–®ç…§ç‰‡ï¼‰")
leave_file = st.file_uploader("Excel/CSVï¼ˆéœ€å«ï¼šsid/å­¸è™Ÿã€start/é–‹å§‹ã€end/çµæŸï¼‰", type=["xlsx","xls","csv"])
leave_imgs = st.file_uploader("å‡å–®ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="leave_imgs")

# æŒ‰éˆ•
btn_ocr = st.button("ğŸ–‡ï¸ OCR + è§£æ")
btn_compare = st.button("ğŸ” æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨")

# session
for key in ["df_guard","df_squad","df_leave_from_imgs","df_leave","five_check"]:
    if key not in st.session_state:
        st.session_state[key] = pd.DataFrame()

# OCR + è§£æ
if btn_ocr:
    if not guard_imgs and not squad_imgs and not leave_imgs:
        st.warning("è«‹è‡³å°‘ä¸Šå‚³ä¸€å¼µç°½å‡ºå…¥æˆ–å‡å–®ç…§ç‰‡ã€‚")
    else:
        reader = load_ocr_reader()
        # å¤§é–€
        rows_g = []
        for f in (guard_imgs or []):
            lines = ocr_image(reader, f)
            df_g = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year, source_label="è­¦è¡›éšŠ")
            if not df_g.empty: rows_g.append(df_g)
        st.session_state.df_guard = pd.concat(rows_g, ignore_index=True) if rows_g else pd.DataFrame()
        # éšŠéƒ¨
        rows_s = []
        for f in (squad_imgs or []):
            lines = ocr_image(reader, f)
            df_s = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year, source_label="ä¸­éšŠ")
            if not df_s.empty: rows_s.append(df_s)
        st.session_state.df_squad = pd.concat(rows_s, ignore_index=True) if rows_s else pd.DataFrame()
        # å‡å–®ç…§ç‰‡ â†’ å€é–“
        rows_l = []
        for f in (leave_imgs or []):
            lines = ocr_image(reader, f)
            dfL = parse_leave_from_lines(lines, fallback_roc_year=default_roc_year)
            if not dfL.empty: rows_l.append(dfL)
        st.session_state.df_leave_from_imgs = pd.concat(rows_l, ignore_index=True) if rows_l else pd.DataFrame()
        st.success(
            f"OCR å®Œæˆï¼šè­¦è¡›éšŠ {len(st.session_state.df_guard)} ç­†ï¼Œä¸­éšŠ {len(st.session_state.df_squad)} ç­†ï¼Œå‡å–®(ç…§ç‰‡) {len(st.session_state.df_leave_from_imgs)} ç­†ã€‚"
        )

# é¡¯ç¤º OCR çµæœ
if not st.session_state.df_guard.empty or not st.session_state.df_squad.empty:
    st.subheader("ğŸ” OCR çµæœï¼ˆç°½å‡ºå…¥ï¼Œå«æ™‚é–“ï¼‰")
    df_view = pd.concat([st.session_state.df_guard, st.session_state.df_squad], ignore_index=True)
    st.dataframe(df_view, use_container_width=True)

# è®€å‡å–® Excel/CSV
if leave_file:
    try:
        st.session_state.df_leave = load_leave_excel(leave_file, fallback_roc_year=default_roc_year)
        st.success(f"å·²è¼‰å…¥å‡å–®ï¼ˆExcel/CSVï¼‰ï¼š{len(st.session_state.df_leave)} ç­†")
    except Exception as e:
        st.error(f"å‡å–®è®€å–å¤±æ•—ï¼š{e}")

# æ¯”å° & åŒ¯å‡º
if btn_compare:
    if st.session_state.df_guard.empty and st.session_state.df_squad.empty and st.session_state.df_leave.empty and st.session_state.df_leave_from_imgs.empty:
        st.warning("è«‹å…ˆåŸ·è¡Œ OCR æˆ–ä¸Šå‚³å‡å–® Excelã€‚")
    else:
        df_leave_final = st.session_state.df_leave.copy() if not st.session_state.df_leave.empty \
            else st.session_state.df_leave_from_imgs.copy()
        five = build_five_checks(st.session_state.df_guard, st.session_state.df_squad, df_leave_final)
        st.session_state.five_check = five

        st.subheader("âœ… äº”æ¬„æª¢æ ¸è¡¨ï¼ˆV=æœ‰è¨˜éŒ„ / X=ç¼ºï¼‰")
        if not five.empty:
            st.dataframe(five, use_container_width=True)
        else:
            st.info("å°šç„¡å¯ç”¢å‡ºçš„äº”æ¬„æª¢æ ¸è³‡æ–™ã€‚")

        # åˆ†é–‹çµ±è¨ˆ
        if not five.empty:
            n1_out = int((five["éšŠéƒ¨ç°½å‡º"]=="X").sum())
            n1_in  = int((five["éšŠéƒ¨ç°½å…¥"]=="X").sum())
            n2_out = int((five["å¤§é–€ç°½å‡º"]=="X").sum())
            n2_in  = int((five["å¤§é–€ç°½å…¥"]=="X").sum())
            n3     = int((five["ç´™æœ¬å‡å–®"]=="X").sum())
        else:
            n1_out = n1_in = n2_out = n2_in = n3 = 0

        st.subheader("ğŸ“Š åˆ†é …çµ±è¨ˆ")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("æœªç°½å‡ºï¼ˆéšŠéƒ¨ï¼‰", n1_out)
        c2.metric("æœªç°½å…¥ï¼ˆéšŠéƒ¨ï¼‰", n1_in)
        c3.metric("æœªç°½å‡ºï¼ˆå¤§é–€ï¼‰", n2_out)
        c4.metric("æœªç°½å…¥ï¼ˆå¤§é–€ï¼‰", n2_in)
        c5.metric("æœªäº¤å‡å–®", n3)

        out_bytes = build_download_excel({
            "äº”æ¬„æª¢æ ¸": five,
            "è­¦è¡›éšŠ_OCR": st.session_state.df_guard,
            "ä¸­éšŠ_OCR": st.session_state.df_squad,
            "å‡å–®æ¸…å–®": df_leave_final
        })
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨ï¼ˆå«äº”æ¬„æª¢æ ¸ï¼‰",
            data=out_bytes,
            file_name=f"å·®å‡ç®¡ç†å“¡_äº”æ¬„æª¢æ ¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

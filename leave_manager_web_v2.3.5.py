# -*- coding: utf-8 -*-
import sys, traceback
import streamlit as st

# -------- Health Checkï¼ˆæœ€å…ˆç•«é¢å°±æœƒå‡ºç¾ï¼‰ --------
st.set_page_config(page_title="å·®å‡ç®¡ç†å“¡ Web v2.3.5", layout="wide")
st.title("ğŸ“‹ å·®å‡ç®¡ç†å“¡ï¼ˆWeb v2.3.5ï¼‰ï½œç›¸ç‰‡ï¼šç°½å‡ºå…¥ + å‡å–® â†’ Excel å ±è¡¨")

with st.expander("ğŸ©º ç’°å¢ƒå¥åº·æª¢æŸ¥ï¼ˆè‹¥å‡ºç¾ç©ºç™½è«‹å…ˆçœ‹é€™è£¡ï¼‰", expanded=True):
    st.write("Python:", sys.version)
    # è©¦è‘—åŒ¯å…¥é—œéµå¥—ä»¶ï¼Œè‹¥å¤±æ•—æœƒå³æ™‚é¡¯ç¤ºéŒ¯èª¤ï¼Œä¸æœƒç™½ç•«é¢
    errors = []
    try:
        import numpy as np
    except Exception as e:
        errors.append(("numpy", e))
    try:
        import pandas as pd
    except Exception as e:
        errors.append(("pandas", e))
    try:
        import PIL
        from PIL import Image
    except Exception as e:
        errors.append(("Pillow", e))
    try:
        import rapidocr_onnxruntime
        from rapidocr_onnxruntime import RapidOCR
    except Exception as e:
        errors.append(("rapidocr-onnxruntime", e))
    try:
        import openpyxl
    except Exception as e:
        errors.append(("openpyxl", e))

    if errors:
        st.error("âŒ ä¸‹é¢å¥—ä»¶è¼‰å…¥å¤±æ•—ï¼ˆè«‹å°ç…§ requirements.txtï¼‰ï¼š")
        for name, e in errors:
            st.code(f"[{name}] {repr(e)}\n{traceback.format_exc(limit=2)}")
        st.stop()
    else:
        st.success("âœ… ä¸»è¦å¥—ä»¶è¼‰å…¥æ­£å¸¸ã€‚")

# -------- ä¹‹å¾Œæ‰è¼‰å…¶é¤˜æ¨¡çµ„èˆ‡å‡½å¼ --------
import re
from io import BytesIO
from typing import List, Dict, Tuple, Optional
from datetime import datetime, date, timedelta

import pandas as pd
import numpy as np
from PIL import Image
from rapidocr_onnxruntime import RapidOCR

# ========================
# OCR backend
# ========================
@st.cache_resource(show_spinner=True)
def load_ocr_reader():
    return RapidOCR()  # CPU ç‰ˆ

def ocr_image(reader, file) -> List[str]:
    img = Image.open(file).convert("RGB")
    arr = np.array(img)
    try:
        result, _ = reader(arr)
    except Exception:
        result = []
    lines = []
    for item in (result or []):
        if not item or len(item) < 3:
            continue
        text = str(item[1]); score = float(item[2] or 0.0)
        if score >= 0.35:
            text = norm(text)
            if text:
                lines.append(text)
    return lines

# ========================
# Utils
# ========================
FULLWIDTH_DIGITS = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
CHECK_MARKS = ["âœ“", "âœ”", "âœ…", "â˜‘", "â– ", "â–¡", "V", "v", "å‹¾", "âˆš"]

def norm(s: str) -> str:
    if s is None: return ""
    s = str(s).translate(FULLWIDTH_DIGITS)
    s = s.replace("\u3000", " ").replace("ï¼", "/")
    s = re.sub(r"[ \t]+", " ", s).strip()
    return s

def has_check(s: str) -> bool:
    return any(m in s for m in CHECK_MARKS)

def find_time(s: str) -> str:
    x = s.replace("ï¼š", ":")
    m = re.search(r"(\d{1,2})\s*[æ™‚ç‚¹é»]\s*(\d{1,2})\s*åˆ†?", x)
    if m:
        hh = int(m.group(1)); mm = int(m.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    m2 = re.search(r"\b(\d{1,2})\s*:\s*(\d{1,2})\b", x)
    if m2:
        hh = int(m2.group(1)); mm = int(m2.group(2))
        if 0 <= hh <= 23 and 0 <= mm <= 59:
            return f"{hh:02d}:{mm:02d}"
    return ""

def roc_to_date(roc_str: str, fallback_roc_year: Optional[int]=None) -> Optional[date]:
    s = norm(roc_str).replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    s = re.sub(r"[.\-]", "/", s)
    m = re.search(r"(\d{2,3})\s*/\s*(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m:
        y, mo, da = map(int, m.groups())
        try:
            return date(y + 1911, mo, da)
        except Exception:
            return None
    m2 = re.search(r"(\d{1,2})\s*/\s*(\d{1,2})", s)
    if m2 and fallback_roc_year:
        mo, da = map(int, m2.groups())
        try:
            return date(fallback_roc_year + 1911, mo, da)
        except Exception:
            return None
    return None

def date_to_roc(d: date) -> str:
    return f"{d.year - 1911}/{d.month:02d}/{d.day:02d}"

def parse_mapping_txt(txt: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    id2name, name2id = {}, {}
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        parts = re.split(r"[\t,ï¼Œ\s]+", ln)
        if len(parts) < 2: continue
        a, b = parts[0], parts[1]
        if re.fullmatch(r"\d{7}", a) and not re.fullmatch(r"\d{7}", b):
            sid, name = a, b
        elif re.fullmatch(r"\d{7}", b) and not re.fullmatch(r"\d{7}", a):
            sid, name = b, a
        else:
            sid, name = (a, b) if re.search(r"\d", a) else (b, a)
        sid, name = norm(sid), norm(name)
        if re.fullmatch(r"\d{7}", sid) and name:
            id2name[sid] = name
            name2id[name] = sid
    return id2name, name2id

# ---- å‡åˆ¥ï¼ˆå¯æ“´å……ï¼‰----
DEFAULT_TYPE_PATTERNS = {
    "ç—…":   [r"ç—…"],
    "äº‹":   [r"äº‹"],
    "ç‰¹åˆ¥": [r"ç‰¹åˆ¥", r"ç‰¹ä¼‘", r"ç‰¹(?!è­¦|æ®Š)"],
    "å…¬":   [r"å…¬(å‡|å‡º)?"],
    "è«–":   [r"è«–"],
}

def detect_leave_type(text: str, extra_keywords: list[str]) -> str:
    T = norm(text)
    for canon, regs in DEFAULT_TYPE_PATTERNS.items():
        for rgx in regs:
            if re.search(rgx, T): return canon
    for kw in extra_keywords:
        if kw and kw in T: return kw
    m = re.search(r"([\u4e00-\u9fa5]{1,4})\s*å‡", T)
    if m: return m.group(1)
    m2 = re.search(r"(å–ª|å©š|ç”¢|è£œ|æ…°|ç—…|äº‹|å…¬|ç‰¹|è«–|å…µå½¹)", T)
    if m2: return m2.group(1)
    return ""

def parse_sign_lines(lines: List[str], id2name: Dict[str,str],
                     fallback_roc_year: int, source_label: str,
                     extra_type_keywords: list[str]) -> pd.DataFrame:
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue
        sid = ""
        m_sid = re.search(r"(\d{7})", L)
        if m_sid: sid = m_sid.group(1)
        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname: name = mname.group(0)
        d = roc_to_date(L, fallback_roc_year=fallback_roc_year)
        time_ = find_time(L)
        dir_ = ""
        if "å‡º" in L: dir_ = "å‡º"
        if "å…¥" in L: dir_ = "å…¥"
        if not dir_ and has_check(L):
            if re.search(r"å‡º.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å‡º"
            elif re.search(r"å…¥.{0,3}([âœ“âœ”Vvâˆšå‹¾])", L): dir_ = "å…¥"
        kind = detect_leave_type(L, extra_type_keywords)
        if any([sid, name, d, dir_, time_, kind]) or has_check(L):
            rows.append({
                "sid": sid,
                "name": name,
                "date": date_to_roc(d) if d else "",
                "dir": dir_,
                "time": time_,
                "type": kind,
                "source": source_label,
                "raw": L
            })
    df = pd.DataFrame(rows, columns=["sid","name","date","dir","time","type","source","raw"]).drop_duplicates()
    if not df.empty:
        df["name"] = df.apply(lambda r: id2name.get(r["sid"], r["name"]), axis=1)
    return df

def parse_leave_from_lines(lines: List[str], id2name: Dict[str,str],
                           name2id: Dict[str,str], fallback_roc_year: int,
                           extra_type_keywords: list[str]) -> pd.DataFrame:
    rows = []
    for ln in lines:
        L = norm(ln)
        if not L: continue
        sid = ""
        m = re.search(r"(\d{7})", L)
        if m: sid = m.group(1)
        name = id2name.get(sid, "")
        if not name:
            mname = re.search(r"[\u4e00-\u9fa5]{2,4}", L)
            if mname:
                name = mname.group(0)
                sid = sid or name2id.get(name, "")
        text = L.replace("å¹´","/").replace("æœˆ","/").replace("æ—¥","")
        cand = re.findall(r"(\d{2,3}\s*/\s*\d{1,2}\s*/\s*\d{1,2}|\d{1,2}\s*/\s*\d{1,2})", text)
        if len(cand) >= 2:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year)
            d2 = roc_to_date(cand[1], fallback_roc_year=fallback_roc_year)
        elif len(cand) == 1:
            d1 = roc_to_date(cand[0], fallback_roc_year=fallback_roc_year); d2 = d1
        else:
            d1 = d2 = None
        leave_type = detect_leave_type(L, extra_type_keywords)
        if sid and d1 and d2:
            rows.append({"sid": sid, "name": name, "start": d1, "end": d2, "type": leave_type, "raw": L})
    return pd.DataFrame(rows).drop_duplicates()

def expand_leave_days(df_leave: pd.DataFrame) -> Dict[str, List[date]]:
    mp: Dict[str, List[date]] = {}
    if df_leave.empty: return mp
    for _, r in df_leave.iterrows():
        if not r["sid"] or pd.isna(r["start"]) or pd.isna(r["end"]): continue
        cur = r["start"]
        while cur <= r["end"]:
            mp.setdefault(r["sid"], []).append(cur)
            cur += timedelta(days=1)
    return mp

def build_five_checks(df_guard: pd.DataFrame, df_squad: pd.DataFrame, df_leave: pd.DataFrame) -> pd.DataFrame:
    keys = set()
    for df in [df_guard, df_squad]:
        if df.empty: continue
        for _, r in df.iterrows():
            if r.get("date") and r.get("sid"):
                keys.add((r["date"], r["sid"], r.get("name","")))
    if not df_leave.empty:
        for _, r in df_leave.iterrows():
            d = r["start"]
            while d <= r["end"]:
                keys.add((date_to_roc(d), r["sid"], r.get("name","")))
                d += timedelta(days=1)
    rows = [{
        "æ—¥æœŸ(ROC)": droc,
        "å­¸è™Ÿ": sid,
        "å§“å": name,
        "éšŠéƒ¨ç°½å‡º": "X",
        "éšŠéƒ¨ç°½å…¥": "X",
        "ç´™æœ¬å‡å–®": "X",
        "å¤§é–€ç°½å‡º": "X",
        "å¤§é–€ç°½å…¥": "X",
    } for (droc, sid, name) in sorted(keys)]
    table = pd.DataFrame(rows)
    def lookup_name(sid):
        for df in (df_guard, df_squad):
            t = df[df["sid"]==sid]["name"]
            if not t.empty: return t.iloc[0]
        return ""
    if not table.empty:
        table["å§“å"] = table.apply(lambda r: r["å§“å"] or lookup_name(r["å­¸è™Ÿ"]), axis=1)
    def mark(df, col_out, col_in):
        if df.empty: return
        for _, r in df.iterrows():
            m = (table["æ—¥æœŸ(ROC)"] == r["date"]) & (table["å­¸è™Ÿ"] == r["sid"])
            if r.get("dir") == "å‡º":
                table.loc[m, col_out] = "V"
            elif r.get("dir") == "å…¥":
                table.loc[m, col_in] = "V"
    mark(df_squad, "éšŠéƒ¨ç°½å‡º", "éšŠéƒ¨ç°½å…¥")
    mark(df_guard, "å¤§é–€ç°½å‡º", "å¤§é–€ç°½å…¥")
    if not df_leave.empty and not table.empty:
        daymap = expand_leave_days(df_leave)
        for i, r in table.iterrows():
            sid = r["å­¸è™Ÿ"]; d_ = roc_to_date(r["æ—¥æœŸ(ROC)"])
            if sid in daymap and d_ and any(x == d_ for x in daymap[sid]):
                table.loc[i, "ç´™æœ¬å‡å–®"] = "V"
    return table.sort_values(["æ—¥æœŸ(ROC)", "å­¸è™Ÿ"])

def build_download_excel(dfs: Dict[str, pd.DataFrame]) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as w:
        for name, df in dfs.items():
            if df is None or df.empty: continue
            df.to_excel(w, index=False, sheet_name=name[:31] or "Sheet1")
    bio.seek(0)
    return bio.read()

# ========================
# UI
# ========================
with st.expander("â„¹ï¸ ä½¿ç”¨èªªæ˜", expanded=False):
    st.markdown("""
1. ä¸Šå‚³ `name_id_clean.txt`ï¼ˆå­¸è™Ÿâ†”å§“åï¼›ç©ºç™½/é€—è™Ÿ/Tab çš†å¯ï¼‰ã€‚
2. ä¸Šå‚³ç°½å‡ºå…¥ç…§ç‰‡ï¼šå·¦ã€Œè­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ã€ã€å³ã€Œç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ã€ã€‚
3. ä¸Šå‚³ç´™æœ¬å‡å–®ç…§ç‰‡ã€‚
4. å´æ¬„å¯è¨­å®šé è¨­æ°‘åœ‹å¹´ï¼Œä¸¦å¯è‡ªè¨‚å‡åˆ¥é—œéµå­—ã€‚
5. é»ã€ŒOCR + è§£æã€â†’ã€Œæ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨ã€â†’ ä¸‹è¼‰ Excelã€‚
""")

st.sidebar.header("æ—¥æœŸèˆ‡å‡åˆ¥è¨­å®š")
default_roc_year = st.sidebar.number_input("é è¨­æ°‘åœ‹å¹´ï¼ˆé‡åˆ°åªæœ‰ MM/DD æ™‚è£œå¹´ï¼‰", min_value=110, max_value=200, value=114)
st.sidebar.subheader("å‡åˆ¥é—œéµå­—ï¼ˆå¯è‡ªè¨‚ï¼‰")
custom_type_str = st.sidebar.text_input("é€—è™Ÿåˆ†éš”ï¼šå¦‚ å–ª,å©š,æ…°,è£œä¼‘,ç”¢,å…µå½¹", value="")
EXTRA_TYPE_KEYWORDS = [norm(x) for x in re.split(r"[,ï¼Œ\s]+", custom_type_str) if norm(x)]

st.header("ğŸ‘¥ ä¸Šå‚³å­¸è™Ÿå§“åå°ç…§è¡¨ï¼ˆTXTï¼‰")
mapping_file = st.file_uploader("name_id_clean.txt", type=["txt"])
id2name, name2id = {}, {}
if mapping_file:
    content = mapping_file.read().decode("utf-8")
    id2name, name2id = parse_mapping_txt(content)
    st.success(f"å·²è¼‰å…¥åå–®ï¼š{len(id2name)} ç­†")

st.header("ğŸ§¾ ä¸Šå‚³ç°½å‡ºå…¥ç…§ç‰‡")
c1, c2 = st.columns(2)
with c1:
    guard_imgs = st.file_uploader("è­¦è¡›éšŠï¼ˆå¤§é–€ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="guard")
with c2:
    squad_imgs = st.file_uploader("ç ”ç©¶ç”Ÿä¸­éšŠï¼ˆéšŠéƒ¨ï¼‰ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="squad")

st.header("ğŸ“‘ ä¸Šå‚³ç´™æœ¬å‡å–®ç…§ç‰‡")
leave_imgs = st.file_uploader("å‡å–®ç…§ç‰‡ï¼ˆå¯å¤šå¼µï¼‰", type=["jpg","jpeg","png"], accept_multiple_files=True, key="leave_imgs")

btn_ocr = st.button("ğŸ–‡ï¸ OCR + è§£æ")
btn_compare = st.button("ğŸ” æ¯”å°ä¸¦ç”¢ç”Ÿå ±è¡¨")

for key in ["df_guard","df_squad","df_leave","five_check"]:
    if key not in st.session_state:
        st.session_state[key] = pd.DataFrame()

if btn_ocr:
    if not guard_imgs and not squad_imgs and not leave_imgs:
        st.warning("è«‹è‡³å°‘ä¸Šå‚³ä¸€å¼µç°½å‡ºå…¥æˆ–å‡å–®ç…§ç‰‡ã€‚")
    else:
        reader = load_ocr_reader()
        rows_g, rows_s, rows_l = [], [], []

        for f in (guard_imgs or []):
            lines = ocr_image(reader, f)
            df_g = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year,
                                    source_label="è­¦è¡›éšŠ", extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not df_g.empty: rows_g.append(df_g)
        st.session_state.df_guard = pd.concat(rows_g, ignore_index=True) if rows_g else pd.DataFrame()

        for f in (squad_imgs or []):
            lines = ocr_image(reader, f)
            df_s = parse_sign_lines(lines, id2name, fallback_roc_year=default_roc_year,
                                    source_label="ä¸­éšŠ", extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not df_s.empty: rows_s.append(df_s)
        st.session_state.df_squad = pd.concat(rows_s, ignore_index=True) if rows_s else pd.DataFrame()

        for f in (leave_imgs or []):
            lines = ocr_image(reader, f)
            dfL = parse_leave_from_lines(lines, id2name, name2id,
                                         fallback_roc_year=default_roc_year,
                                         extra_type_keywords=EXTRA_TYPE_KEYWORDS)
            if not dfL.empty: rows_l.append(dfL)
        st.session_state.df_leave = pd.concat(rows_l, ignore_index=True) if rows_l else pd.DataFrame()

        st.success(
            f"OCR å®Œæˆï¼šè­¦è¡›éšŠ {len(st.session_state.df_guard)} ç­†ï¼Œä¸­éšŠ {len(st.session_state.df_squad)} ç­†ï¼Œå‡å–® {len(st.session_state.df_leave)} ç­†ã€‚"
        )

if not st.session_state.df_guard.empty or not st.session_state.df_squad.empty:
    st.subheader("ğŸ” OCR çµæœï¼ˆç°½å‡ºå…¥ï¼Œå«æ™‚é–“èˆ‡å‡åˆ¥ï¼‰")
    st.dataframe(pd.concat([st.session_state.df_guard, st.session_state.df_squad], ignore_index=True),
                 use_container_width=True)

if not st.session_state.df_leave.empty:
    st.subheader("ğŸ“„ å‡å–®ï¼ˆç…§ç‰‡ï¼‰è§£æçµæœï¼ˆå«å‡åˆ¥ï¼‰")
    st.dataframe(st.session_state.df_leave, use_container_width=True)

if btn_compare:
    if st.session_state.df_guard.empty and st.session_state.df_squad.empty and st.session_state.df_leave.empty:
        st.warning("è«‹å…ˆåŸ·è¡Œ OCRã€‚")
    else:
        five = build_five_checks(st.session_state.df_guard, st.session_state.df_squad, st.session_state.df_leave)
        st.session_state.five_check = five

        st.subheader("âœ… äº”æ¬„æª¢æ ¸è¡¨ï¼ˆV=æœ‰è¨˜éŒ„ / X=ç¼ºï¼‰")
        if not five.empty:
            st.dataframe(five, use_container_width=True)
        else:
            st.info("å°šç„¡å¯ç”¢å‡ºçš„äº”æ¬„æª¢æ ¸è³‡æ–™ã€‚")

        if not five.empty:
            n1_out = int((five["éšŠéƒ¨ç°½å‡º"]=="X").sum())
            n1_in  = int((five["éšŠéƒ¨ç°½å…¥"]=="X").sum())
            n2_out = int((five["å¤§é–€ç°½å‡º"]=="X").sum())
            n2_in  = int((five["å¤§é–€ç°½å…¥"]=="X").sum())
            n3     = int((five["ç´™æœ¬å‡å–®"]=="X").sum())
        else:
            n1_out = n1_in = n2_out = n2_in = n3 = 0

        st.subheader("ğŸ“Š åˆ†é …çµ±è¨ˆ")
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("æœªç°½å‡ºï¼ˆéšŠéƒ¨ï¼‰", n1_out)
        c2.metric("æœªç°½å…¥ï¼ˆéšŠéƒ¨ï¼‰", n1_in)
        c3.metric("æœªç°½å‡ºï¼ˆå¤§é–€ï¼‰", n2_out)
        c4.metric("æœªç°½å…¥ï¼ˆå¤§é–€ï¼‰", n2_in)
        c5.metric("æœªäº¤å‡å–®", n3)

        out_bytes = build_download_excel({
            "äº”æ¬„æª¢æ ¸": five,
            "è­¦è¡›éšŠ_OCR": st.session_state.df_guard,
            "ä¸­éšŠ_OCR": st.session_state.df_squad,
            "å‡å–®æ¸…å–®": st.session_state.df_leave
        })
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å ±è¡¨ï¼ˆå«äº”æ¬„æª¢æ ¸ï¼‰",
            data=out_bytes,
            file_name=f"å·®å‡ç®¡ç†å“¡_äº”æ¬„æª¢æ ¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
